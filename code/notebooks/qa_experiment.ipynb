{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scriptinit\n",
    "import random\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.ifelse import ifelse\n",
    "from models import *\n",
    "from data_import import *\n",
    "from util import *\n",
    "import layers\n",
    "import optimizers\n",
    "from experiment import *\n",
    "from os.path import join\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn on for debugging\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    theano.config.optimizer='fast_compile'\n",
    "    theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables that don't change between experiments/trials\n",
    "constants = {\n",
    "    'datadir': 'data/',\n",
    "    'glovedir': 'data/glove',\n",
    "    'report_wait': 500,\n",
    "    'save_wait': 1000,\n",
    "    'wv_dimensions': 50,  # speed up learning by using the smallest GloVe dimension\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step up argument parsing\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-tn', '--task_number', type=int, required=True)\n",
    "parser.add_argument('-lr', '--base_lr', type=float, required=True)\n",
    "parser.add_argument('-mt', '--model_type', type=str, required=True)\n",
    "parser.add_argument('-hd', '--hidden_dim', type=int, required=True)\n",
    "parser.add_argument('-l2', '--l2_reg', type=float, required=True)\n",
    "\n",
    "parser.add_argument('-lhd', '--lstm_hidden_dim', type=int, required=True)\n",
    "parser.add_argument('-mp', '--mean_pool', type=int, required=True)\n",
    "parser.add_argument('-log', '--logging_path', type=str, required=True)\n",
    "\n",
    "parser.add_argument('-me', '--max_epochs', type=int, required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variables that change between runs\n",
    "if util.in_ipython():\n",
    "    constants = {\n",
    "        'datadir': '../../data/',\n",
    "        'glovedir': '../../data/glove',\n",
    "        'report_wait': 100,\n",
    "        'save_wait': 1000,\n",
    "        'max_epochs': 100,\n",
    "        'wv_dimensions': 50,  # speed up learning by using the smallest GloVe dimension\n",
    "    }\n",
    "\n",
    "    hyperparams = {\n",
    "        # data and logging\n",
    "        'task_number': 1,  # {1, 3, 5, 17, 19}\n",
    "\n",
    "        # HYPERPARAMETERS\n",
    "        'base_lr': 1e-2,\n",
    "\n",
    "        # all of the models\n",
    "        'model_type': 'sentenceEmbedding',  # one of |sentenceEmbedding| or |averaging|\n",
    "        'hidden_dim': 128,\n",
    "        'l2_reg': 1e-5,\n",
    "\n",
    "\n",
    "        # specific to sentence embedding model\n",
    "        'lstm_hidden_dim': 128,\n",
    "        'mean_pool': False,\n",
    "        \n",
    "        'logging_path': 'logging_dir', \n",
    "    }\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "    hyperparams = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load into namespace and log to metadata\n",
    "for var, val in hyperparams.iteritems():\n",
    "    exec(\"{0} = hyperparams['{0}']\".format(var))\n",
    "    util.metadata(var, val)\n",
    "\n",
    "for var, val in constants.iteritems():\n",
    "    exec(\"{0} = constants['{0}']\".format(var))\n",
    "    util.metadata(var, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_ex, _ = get_data(datadir, task_number, test=False)\n",
    "word_vectors = wordVectors(train_ex)\n",
    "train_ex = examples_to_example_ind(word_vectors, train_ex)  # get examples in numerical format\n",
    "\n",
    "# shuffle data\n",
    "random.shuffle(train_ex)\n",
    "\n",
    "# split the train into 90% train, 10% dev\n",
    "train = train_ex[:int(.9 * len(train_ex))]\n",
    "dev = train_ex[int(.9 * len(train_ex)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get word_vectors (Using glove for now)\n",
    "wv_matrix = word_vectors.get_wv_matrix(wv_dimensions, glovedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial and setup the attention layer\n",
    "r = 0.001\n",
    "attention_embedding = np.random.rand(hidden_dim, wv_matrix.shape[1]) * 2 * r - r\n",
    "\n",
    "attention_model = attentionModel(embeddings=attention_embedding, lstm_hidden_dim=lstm_hidden_dim)\n",
    "\n",
    "# set up the question + facts -> answer model\n",
    "if model_type == 'averaging':\n",
    "    qa_model = averagingModel(wv_matrix, hidden_dim=hidden_dim, num_classes=wv_matrix.shape[1])\n",
    "elif model_type == 'sentenceEmbedding':\n",
    "    qa_model = embeddingModel(wv_matrix, lstm_hidden_dim=lstm_hidden_dim, nn_hidden_dim=hidden_dim,\n",
    "                            num_classes=wv_matrix.shape[1], mean_pool=mean_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate answer probabilities and predictions\n",
    "support = T.imatrix()\n",
    "mask = T.imatrix()\n",
    "question_idxs = T.ivector()\n",
    "hints = T.ivector()\n",
    "\n",
    "# estimate relevance of each sentence\n",
    "relevance_probs = attention_model.get_relevance_probs(support, mask, question_idxs)\n",
    "\n",
    "def get_word_idxs(relevant_sentence_idxs, support_, mask_):\n",
    "    rel_support = support_[relevant_sentence_idxs, :]\n",
    "    rel_mask = mask_[relevant_sentence_idxs, :]\n",
    "    return rel_support[rel_mask.nonzero()].ravel()\n",
    "\n",
    "# By default, the attention model retrieves any sentence with prob > 0.5 under the model\n",
    "# If no sentence exists, it returns the top two sentences in chronological order\n",
    "max_idxs = T.sort(T.argsort(relevance_probs[:, 1])[-2:])  \n",
    "prob_idxs = T.arange(relevance_probs.shape[0])[T.nonzero(relevance_probs[:, 1] > 0.5)]\n",
    "est_idxs = ifelse(T.lt(T.sum(relevance_probs[:, 1] > 0.5), 1), max_idxs, prob_idxs)\n",
    "\n",
    "\n",
    "# joint training of question model + attention model\n",
    "est_rel_facts = get_word_idxs(est_idxs, support, mask)\n",
    "answer_probs = qa_model.get_answer_probs(est_rel_facts, question_idxs)\n",
    "\n",
    "# train the qa-model using the hints\n",
    "# true_rel_facts = get_word_idxs(hints.nonzero(), support, mask)\n",
    "# answer_probs = qa_model.get_answer_probs(true_rel_facts, question_idxs)\n",
    "\n",
    "# predict without using the hint\n",
    "answer_pred = T.argmax(qa_model.get_answer_probs(est_rel_facts, question_idxs))\n",
    "#answer_pred = T.argmax(qa_model.get_answer_probs(true_rel_facts, question_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the loss and cost function\n",
    "answer = T.iscalar()\n",
    "qa_nll = -T.log(answer_probs)[0, answer]\n",
    "attention_nll = -T.sum(T.log(relevance_probs)[T.arange(hints.shape[0]), hints])\n",
    "loss = qa_nll + attention_nll\n",
    "cost = loss + l2_reg * layers.l2_penalty(qa_model.params + attention_model.params)\n",
    "param_norms = layers.l2_penalty(qa_model.params + attention_model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimization\n",
    "updates = optimizers.RMSProp(cost, qa_model.params + attention_model.params, base_lr=base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile functions to train and evaluate the model\n",
    "print 'Compiling predict function'\n",
    "qa_model.predict = theano.function(\n",
    "                   inputs = [support, mask, question_idxs],\n",
    "                   outputs = answer_pred)\n",
    "\n",
    "print 'Compiling objective function'\n",
    "qa_model.objective = theano.function(\n",
    "                    inputs = [support, mask, question_idxs, answer, hints],\n",
    "                    outputs = loss)\n",
    "\n",
    "print 'Compiling backprop function'\n",
    "qa_model.backprop = theano.function(\n",
    "                    inputs=[support, mask, question_idxs, answer, hints],\n",
    "                    outputs=[loss, answer_probs, qa_nll, attention_nll],\n",
    "                    updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the experiment object\n",
    "controllers = [BasicController(report_wait=report_wait, save_wait=save_wait, max_epochs=max_epochs, path=logging_path)]\n",
    "\n",
    "dset_samples = len(dev)\n",
    "observers = [ObjectiveObserver(dset_samples=dset_samples, report_wait=report_wait),\n",
    "             AccuracyObserver(dset_samples=dset_samples, report_wait=report_wait)]\n",
    "\n",
    "experiment = Experiment(qa_model, train, dev, controllers=controllers, observers=observers, path=logging_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# launch the experiment\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot learning curves\n",
    "#report(join(logging_path, 'history.cpkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
