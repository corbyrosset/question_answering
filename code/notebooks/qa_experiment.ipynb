{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import random\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from models import *\n",
    "from data_import import *\n",
    "from util import *\n",
    "import layers\n",
    "import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "datadir = '../../data/'\n",
    "glovedir = '../../data/glove'\n",
    "SEED = 999\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "task_number = 1 # simple task for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn on for debugging\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    theano.config.optimizer='fast_compile'\n",
    "    theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "wv_dimensions = 50  # speed up learning by using the smallest GloVe dimension\n",
    "\n",
    "# training\n",
    "num_epochs = 100\n",
    "base_lr = 1e-2\n",
    "\n",
    "## all of the models\n",
    "model_type = 'sentenceEmbedding'  # one of |sentenceEmbedding| or |averaging|\n",
    "hidden_dim = 128\n",
    "l2_reg = 0.0\n",
    "\n",
    "\n",
    "# specific to sentence embedding model\n",
    "lstm_hidden_dim = 128\n",
    "mean_pool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_ex, _ = get_data(datadir, task_number, test=False)\n",
    "word_vectors = wordVectors(train_ex)\n",
    "train_ex = examples_to_example_ind(word_vectors, train_ex)  # get examples in numerical format\n",
    "\n",
    "# shuffle data\n",
    "random.shuffle(train_ex)\n",
    "\n",
    "# split the train into 70% train, 30% dev\n",
    "train = train_ex[:int(.9 * len(train_ex))]\n",
    "dev = train_ex[int(.1 * len(train_ex)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get word_vectors (Using glove for now)\n",
    "wv_matrix = word_vectors.get_wv_matrix(wv_dimensions, glovedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the basic model\n",
    "if model_type == 'averaging':\n",
    "    model = averagingModel(wv_matrix, hidden_dim=hidden_dim, num_classes=wv_matrix.shape[1])\n",
    "elif model_type == 'sentenceEmbedding':\n",
    "    model = embeddingModel(wv_matrix, lstm_hidden_dim=lstm_hidden_dim, nn_hidden_dim=hidden_dim,\n",
    "                            num_classes=wv_matrix.shape[1], mean_pool=mean_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate answer probabilities and predictions\n",
    "support_idxs = T.ivector()\n",
    "question_idxs = T.ivector()\n",
    "\n",
    "answer_probs = model.get_answer_probs(support_idxs, question_idxs)\n",
    "answer_pred = T.argmax(answer_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the loss and cost function\n",
    "answer = T.ivector()\n",
    "loss = -T.mean(T.log(answer_probs)[T.arange(answer.shape[0]), answer])\n",
    "cost = loss + l2_reg * model.l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimization\n",
    "updates = optimizers.Adagrad(cost, model.params, base_lr=base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile functions to train and evaluate the model\n",
    "print 'Compiling predict function'\n",
    "predict = theano.function(\n",
    "           inputs = [support_idxs, question_idxs],\n",
    "           outputs = answer_pred\n",
    "        )\n",
    "\n",
    "print 'Compiling backprop function'\n",
    "backprop = theano.function(\n",
    "            inputs=[support_idxs, question_idxs, answer],\n",
    "            outputs=[loss, answer_probs],\n",
    "            updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training!!!\n",
    "epoch = 0\n",
    "train_acc_hist = []\n",
    "dev_acc_hist = []\n",
    "while epoch < num_epochs:\n",
    "    print 'Epoch %d ' % epoch\n",
    "    for example in verboserate(train):\n",
    "        backprop(np.concatenate(example.sentences), example.question, example.answer)\n",
    "    \n",
    "    print 'Computing Train/Val Accuracy: '\n",
    "    def compute_acc(dset):\n",
    "        correct = 0.\n",
    "        for example in verboserate(dset):\n",
    "            if example.answer == predict(np.concatenate(example.sentences), example.question):\n",
    "                correct += 1\n",
    "        return correct / float(len(dset))\n",
    "    \n",
    "    train_acc, dev_acc  = compute_acc(train), compute_acc(dev)\n",
    "    train_acc_hist.append(train_acc)\n",
    "    dev_acc_hist.append(dev_acc)\n",
    "    print 'Train Accuracy: %f , Validation Accuracy %f' % (train_acc, dev_acc)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot learning curves\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(train_acc_hist)\n",
    "plt.plot(dev_acc_hist)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend(['Train', 'Dev'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
